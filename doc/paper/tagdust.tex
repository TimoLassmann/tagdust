\documentclass[a4,center,fleqn]{NAR}

% Enter dates of publication
\copyrightyear{2008}
\pubdate{31 July 2009}
\pubyear{2013}
\jvolume{37}
\jissue{12}

%\articlesubtype{This is the article type (optional)}

\begin{document}

\title{TagDust version 2.0 - a program to extract, filter and correctly label next generation sequencing reads.}

\author{%
Timo Lassmann\,$^{1}$%
\footnote{To whom correspondence should be addressed.
Tel: +44 000 0000000; Fax: +44 000 0000000; Email: xxx@yyyy.ac.zz}}

\address{%
$^{1}$Division of Genomic Technologies, Center for Life Science Technologies,
RIKEN Yokohama Institute,
1-7-22 Suehiro-cho, Tsurumi-ku, Yokohama
230-0045 Kanagawa, Japan 
}
% Affiliation must include:
% Department name, institution name, full road and district address,
% state, Zip or postal code, country

\history{%
Received January 1, 2013;
Revised February 1, 2013;
Accepted March 1, 2013}

\maketitle

\begin{abstract}

Arguably the most basic step in the analysis of next generation sequences (NGS) involves extracting mappable reads from the raw reads produced by the various sequencing instruments. The presence of indices, fingerprints, adaptors and artifacts all of which containing sequencing errors makes this step non-trivial. 

Tagdust2 is designed to carry out all steps required to go from raw- to mappable reads. This includes de-multiplexing libraries, removing adaptors, identifying unique molecular identifiers and discarding artifacts. Our method allows users to specify the expected architecture of a read and converts it into a hidden Markov model (HMM). The latter recognized different segments in the raw reads and assigns reads to a particular barcode even in the presence of sequencing errors. Artifactual sequences, which by definition do not not match the architecture are automatically discarded.

\end{abstract}

\section{Introduction}

Next generation sequencing has greatly accelerated the accumulation of genomics data. Different protocols targeting the genome, epi-genome and transcriptions are widely used\cite{ENCODEINTEGRATION}. In essence, all protocols capture biological sequences of interest while necessarily adding adaptors and other sequences to facilitate cost effective sequencing. An obvious examples here is the use of indices or barcodes allowing researchers to profile multiple samples in a single lane\cite{Craig:2008,Kircher:2012}. In addition recent protocols include the addition of random stretches of sequences in order to correct for PCR and other biases \cite{Kivioja:2012kg}. In summary, the number of nucleotides used to describe what is being sequenced is increasing.

As the length of informative nucleotides increases so is the chance that sequencing errors occur in these key sequences. In the best case these errors lead to some sequences being lost to the downstream analysis, but in the worse case sequences can be mixed up between samples leading to analytical noise. An additional complication is that the error rates of current sequencing instruments vary a lot and is not obvious to select an appropriate strategy to work with raw reads. 

We previously released TagDust to remove artifactual reads\cite{Tagdust2009}. While being useful, the focus on identifying and discarding whole reads meant that most processing pipelines using TagDust required many additional programs and steps. Here I describe TagDust2 a program designed to carry out all steps required to go from raw reads to sequences ready to be mapped. The key part of finding the mappable read and annotating it based on barcodes is carried out by a user defined hidden Markov model (HMM). This approach tackles two problems: firstly, a user defined architecture gives great flexibility in respect to the different assays used and secondly HMMs themselves are well suited to work with sequences containing different types of sequencing errors. 

safasf af a 
af a
a f 

sf

a

f 
 
% **************************************************************
% Keep this command to avoid text of first page running into the
% first page footnotes

\enlargethispage{-65.1pt}
% **************************************************************

\begin{figure}[t]
\begin{center}
\includegraphics[scale = 0.5]{../figures/figure1.pdf}
\end{center}
\caption{Overview of the TagDust workflow. Sequences are labelled according to the HMM architecture and relevant information written to the output. 
}
\label{figure1}
\end{figure}


\section{MATERIALS AND METHODS}

\subsection{General Approach}

TagDust implements a small library of HMMs which I will refer to as segments. Each segment contains a silent start and end state which are used to connect multiple segments. Segments are hand-designed to capture commonly occurring features in raw sequences such as the combination of barcodes, variable length sequences and so on. For a complete list see table 1. Users can use a simple command line interface to specify the expected sequence of segments in their reads. TagDust automatically constructs a global HMMs from the segments and starts scoring the individual reads (see Figure \ref{figure1}). 

Parameterization... 
\subsection{Sequence scoring} 

In short read mapping the mapping quality reflects the confidence we have in one particular mapping location over all others. Analogously, TagDust compares the probability of each read matching to the user specified HMM to the total summed probability including a random model. forward.

\begin{equation}
P = 1 - \frac{P(x|M)}{P(x|M) + P(x|R)}
\end{equation} 

In the presence of barcodes $P(x|M)$ the forward algorithm includes the probabilities of matching multiple barcodes. To obtain the probability of the barcode with the highest probability I employ a simple trick: at each segment including 

\begin{equation}
	V = \max_j \left( \frac{f_s(i)  ( a_{s,m_j} e_{m_j}(x+1) b_{m_j}(i+1)}{\sum\limits_{\pi} P(x,\pi | M )}\right)
\end{equation}
where $m_j$ is the first match states of a barcode HMM $j$.  
 

\subsection{Optimal accuracy decoding} 

To obtain the most probable labeling of the sequence, I employ the optimal accuracy decoding algorithm as described in \citep{Kall:2005vg}. To apply this algorithm to our problem define the label probability of a nucleotide by the summed posterior label probabilities of all states belonging to a particular HMM segment. A secondary dynamic programming algorithm is used to determine the path with the maximal label probability, constrained by the global HMM architecture. The label probabilities are essentially used as a substitution matrix while the architecture is enforced by the equivalent of gap penalties. 

If fingerprints are present TagDust checks at this stage if the length after decoding matches the users input. If not the read is discarded. 



\subsection{Further read filtering}

TagDust allows users to specify a fasta file containing known sequences the user wishes to exclude from mapping. For example in transcriptome sequencing one commonly wants to remove ribosomal sequences from the downstream analysis. To address this issue in a general manner TagDust exhaustively align all reads to the target reference sequences and discard all reads with less than X mismatches. For efficiency we implemented the Myers bit parallel algorithm\cite{MyersDYN} using SIMD instructions as well as using thread level parallelism. 

\subsection{Filtering low complexity sequences.}
Tagdust implements a simplified version of the DUST module (R. Tatusov and D.J. Lipman, unpublished data) to filter out low complexity reads. While this is not strictly necessary for NGS reads I implemented it to remove poly-A and other "very" low complexity sequences. The algorithm is only applied to the first sixty-four nucleotides of the reads. 

\subsection{Implementation} 

Internally, TagDust uses full profile HMMs for each segment. To emulate different segments we simply set some transition probabilities to zero. For example the read building block 'R' is implemented as a profile HMM with one column and transitions directly to and from the insertion state. We parallelized the most computationally demanding parts using threads. TagDust is written in C, documented and the source code is freely available here: 




\section{RESULTS}

\subsection{Large benchmark}

To assess the accuracy of TagDust in a wide range of possible applications, we simulate datasets varying the number of barcodes used, the barcode lengths and the sequencer error rate and insertion and deletion (InDel) frequencies. In all experiments we simulated 900k sequences containing the barcode but added 100k random sequences which should not be extracted. We compared our results to the program fastx barcode splitter from the fastx package. This program allows for sequencing errors in the barcodes and performs much better than approaches using exact pattern matching. 

In all of the simulated test cases, TagDust mislabeled less than half a percent of the sequences (Figure \ref{figure2}). Regardless of which program is used it is clear that the barcode sequence has to be long to allow unambiugious identification especially when udon g 96 barcodes. 
I recommend using long barcode umbent   When using 96 barcodes the  As expected, in noisy datasets fewer reads are extracted. In contrast, this percentage is usually higher in fastx. In addition, the simple strategy using one mismatch leads fastx to assign between 25 - 100\% of the random sequences to a library. Using default parameters, Tagdust does not extract any reads when using many short barcodes. 

\begin{figure*}[h]
\begin{center}
\includegraphics[scale = 1]{../figures/figure2.pdf}

\end{center}
\caption{Results on large datasets. Left and right plots show the percentage of extracted read and the percentage of mislabeled reads, while top and bottom plots correspond to a simulated insertion / deletion frequency of 10 and 50 percent respectively. Within each plot the number of barcodes (N) and their length (L) are shown on the x axis while the simulated error rate is shown on the y axis. Results for fastx\_barcode\_clipper and TagDust are shown in the top and bottom panel respectively. Red backgrounds indicate that too many reads were extracted (left) and a high rate of mislabeled reads (right). With default parameters Tagdust does not extract any reads in very difficult test cases (shaded in grey). 
}
\label{figure2}
\end{figure*}

\subsection{Extraction rate on read data.}



\begin{table}[h]
\tableparts{%
\caption{Summary of single cell extracted reads.}
\label{table:01}%
}{%
\begin{tabular*}{\columnwidth}{@{}lrr@{}}
\toprule
Description & Number of Reads & Percentage
\\
\colrule
Total reads   & 110622138 & 100.00\%\\
\colrule
Problems with architecture & 8503361 & 7.69\%\\
Low Complexity & 7228724 & 6.53\%\\
Ambiguous barcode & 59574  &0.05\% \\
Too short & 1  & 0.00\%\\ 
\colrule
Extracted & 94830478 & 85.72\%\\
\botrule
\end{tabular*}%
}
{This is a table footnote}
\end{table}
7.69%
6.53%
0.05%
85.72%



\begin{table}[h]
\tableparts{%
\caption{Summary of UMI extracted reads.}
\label{table:01}%
}{%
\begin{tabular*}{\columnwidth}{@{}lrr@{}}
\toprule
Description & Number of Extracted Reads & Percentage
\\
\colrule
15 PCR cyles & 62110757 & 96.0\%\\
25 PCR cyles & 75824331 & 95.1\%\\
\colrule
1000 cells & 87271807 & 95.8\%\\
100 cells & 58146883  & 87.8\% \\
10 cells & 27051690  & 67.2\%\\ 
\botrule
\end{tabular*}%
}
{This is a table footnote}
\end{table}






\begin{figure}[t]
\begin{center}
\includegraphics[scale = 0.5]{../figures/comp_15c_repeat1_25c_repeat1_umi.pdf}
\includegraphics[scale = 0.5]{../figures/comp_15c_repeat1_25c_repeat1_noumi.pdf}
\end{center}
\caption{Caption for wide figure over two columns.
\textbf{(a)} Left figure.
\textbf{(b)} Right figure (see (a)).
}
\label{NAR-fig2}
\end{figure}


\subsection{Results subsection three}



\section{DISCUSSION}

\subsection{Discussion subsection one}



\subsection{Discussion subsection two}




%\begin{figure}[t]
%\begin{center}
%\includegraphics{NAR-fig1.eps}
%\end{center}
%\caption{Caption for figure within column.}
%\label{NAR-fig1}
%\end{figure}

\subsection{Discussion subsection three}

Tagdust2 is both accurate and versatile. 
Text. text. 
Compared to other is wins. 

\section{CONCLUSION}

As wet protocols become more sophisticated and take full advantage of longer read lengths the architecture of raw reads is becoming ever more complex. Tagdust2 is a one stop solution to work with the ever increasing complexity of this step. The ability to support a wide variety of read architectures together with the power of HMMs in matching with errors makes it unique. 

In future I may consider to implement learning from the reads directly. 




\section{ACKNOWLEDGEMENTS}
This work was supported by a Research Grant from the Japanese Ministry of Education, Culture, Sports, Science and Technology (MEXT) to the RIKEN Center for Life Science Technologies.

\subsubsection{Conflict of interest statement.} None declared.
\newpage


\begin{thebibliography}{4}

% Format for article

\bibitem{ENCODEINTEGRATION}
Ian Dunham, {\it et. al.} (2012)
An integrated encyclopedia of DNA elements in the human genome.
\textit{Nature}, \textbf{489},57–74.

\bibitem{Craig:2008}
Craig DW, Pearson JV, Szelinger S, Sekar A, Redman M, Corneveaux JJ, Pawlowski TL, Laub T, Nunn G, Stephan DA, \textit{et al.} (2008)
Identification of genetic variants using bar-coded multiplexed sequencing.
\textit{Nat. Methods}, \textbf{5}, 887-893.

\bibitem{Kircher:2012}
Kircher, M., Sawyer, S., and Meyer, M. (2012).
Double indexing overcomes inaccuracies in multiplex sequencing on the Illumina platform.
\textit{Nucleic acids research}, \textbf{40}(1), e3. 

\bibitem{Kivioja:2012kg}
Kivioja, T., Vähärautio, A., Karlsson, K., Bonke, M., Enge, M., Linnarsson, S., and Taipale, J. (2012).
Counting absolute numbers of molecules using unique molecular identifiers.
\textit{Nature methods}, \textbf{9}(1), 72-74.

\bibitem{Tagdust2009}
Lassmann, T., Hayashizaki, Y., and Daub, C. O. (2009).

TagDust - a program to eliminate artifacts from next generation sequencing data.
\textit{Bioinformatics}, \textbf{25}(21), 2839-2840. 

\bibitem{MyersDYN}
Myers, G. (1999).
A fast bit-vector algorithm for approximate string matching based on dynamic programming.
\textit{Journal of the ACM}, \textbf{46}, 1–13.


% Format for book
\bibitem{2}
Author,D., Author,E.F. and Author,G. (1995)
\textit{Book Title}.
Publisher Name, Publisher Address.

% Format for chapter in book
\bibitem{3}
Author,H. and Author,I. (2005)
Chapter title.
In
Editor,A. and Editor,B. (eds),
\textit{Book Title},
Publisher Name, Publisher Address,
pp.\ 60--80.

% Another article
\bibitem{4}
Author,Y. and Author,Z. (2002)
Article title.
\textit{Abbreviated Journal Name}, \textbf{53}, 500--520.
\bibitem{gaga}
Author,A.B. and Author,C. (1992)
Article title GAGAGAGAGA.
\textit{Abbreviated Journal Name}, \textbf{5}, 300--330.




%@article{Kivioja:2012kg,
%author = {Kivioja, Teemu and V{\"a}h{\"a}rautio, Anna and Karlsson, Kasper and Bonke, Martin and Enge, %Martin and Linnarsson, Sten and Taipale, Jussi},
%title = {{Counting absolute numbers of molecules using unique molecular identifiers.}},
%journal = {Nature methods},
%year = {2012},
%volume = {9},
%number = {1},
%pages = {72--74},
%month = jan
%}


%@book{durbin,
%    author = {Durbin, R. and Eddy, S. and Krogh, A. and Mitchison, G.},
%    edition = {eleventh},
%publisher = {Press, Cambridge U.},
%    title = {{Biological sequence analysis}},
%    year = {2006}
%}



%@article{Kall:2005vg,
%author = {K{\"a}ll, Lukas and Krogh, Anders and Sonnhammer, Erik L L},
%title = {{An HMM posterior decoder for sequence feature prediction that includes homology information}},
%journal = {Bioinformatics (Oxford, England)},
%year = {2005}
%}




\end{thebibliography}

\end{document}
